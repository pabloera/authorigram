# Configurações Gerais do Projeto

# Informações do Projeto
project:
  name: "dataanalysis-bolsonarismo"
  version: "0.2.0"
  description: "Análise de Discurso em Canais do Telegram: Bolsonarismo, Negacionismo e Autoritarismo"

# Configurações de Caminhos
paths:
  data: "data"
  raw: "data/raw"
  processed: "data/processed"
  results: "results"
  models: "models"
  logs: "logs"
  documentation: "documentation"

# Configurações de Processamento
processing:
  chunk_size: 10000
  sample_size: 10000
  encoding: "utf-8"
  memory_limit: "2GB"
  max_workers: 4
  timeout: 3600
  cache_enabled: true
  cache_dir: "data/interim/cache"

# Configurações de Logging
logging:
  default_level: "INFO"
  file_rotation: 30
  log_to_file: true
  log_to_console: true
  format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"

# Visualization removed - project focused on data processing only

# Configurações de Pipeline
pipeline:
  stop_on_error: true
  checkpoint_enabled: true
  
# Configurações Centralizadas Anthropic API para Todos os 13 Stages
anthropic:
  model: "claude-3-5-haiku-latest"  # Modelo especificado pelo usuário
  max_tokens: 4000
  temperature: 0.3
  cost_monitoring: true
  fallback_enabled: true

# Stage 01: Data Validation
data_validation:
  use_anthropic: false  # Manter tradicional por performance
  validation_level: "comprehensive"
  
# Stage 02: Encoding Fix  
encoding_fix:
  use_anthropic: true
  columns_to_fix: ["texto", "text_cleaned", "canal", "hashtags"]
  confidence_threshold: 0.8
  
# Stage 02b: Deduplication
deduplication:
  use_anthropic: true
  text_column: "texto"
  similarity_threshold: 0.9
  semantic_analysis: true
  
# Stage 01b: Feature Validation (Novo Fluxo)
feature_validation:
  use_anthropic: false  # Processamento local
  validate_existing: true
  enrich_basic: true
  
# Stage 01c: Political Analysis (Novo Fluxo)  
political_analysis:
  use_anthropic: true  # API necessária para melhor qualidade
  batch_size: 10
  confidence_threshold: 0.7
  use_cache: true
  
# Stage 01b: Feature Extraction (Legado - Mantido para compatibilidade)
feature_extraction:
  use_anthropic: true
  extract_political_features: true
  extract_emotional_features: true
  
# Stage 03: Text Cleaning - OBRIGATÓRIO API Anthropic
text_cleaning:
  use_anthropic: true  # OBRIGATÓRIO a partir da Stage 3
  text_column: "texto"
  preserve_context: true
  remove_urls: false
  remove_emojis: false
  
# Stage 04: Sentiment Analysis - OBRIGATÓRIO API Anthropic
sentiment:
  use_anthropic: true  # OBRIGATÓRIO a partir da Stage 3
  text_column: "text_cleaned"
  method: "hybrid"
  language: "pt"
  political_context: true
  
# Stage 05: Topic Modeling - OBRIGATÓRIO API Anthropic
lda:
  use_anthropic_interpretation: true  # OBRIGATÓRIO a partir da Stage 3
  n_topics: 15
  iterations: 1000
  alpha: 0.01
  beta: 0.01
  
# Stage 06: TF-IDF Extraction - OBRIGATÓRIO API Anthropic
tfidf:
  use_anthropic: true  # OBRIGATÓRIO a partir da Stage 3
  max_features: 5000
  ngram_range: [1, 3]
  semantic_grouping: true
  
# Stage 07: Clustering - OBRIGATÓRIO API Anthropic
clustering:
  use_anthropic_validation: true  # OBRIGATÓRIO a partir da Stage 3
  method: "kmeans"
  n_clusters: 10
  validation_method: "semantic"
  
# Stage 08: Hashtag Normalization - OBRIGATÓRIO API Anthropic
hashtag_normalization:
  use_anthropic: true  # OBRIGATÓRIO a partir da Stage 3
  min_frequency: 5
  similarity_threshold: 0.8
  semantic_clustering: true
  
# Stage 09: Domain Analysis - OBRIGATÓRIO API Anthropic
domain_analysis:
  use_anthropic: true  # OBRIGATÓRIO a partir da Stage 3
  min_frequency: 5
  batch_size: 30
  credibility_analysis: true
  
# Stage 10: Temporal Analysis - OBRIGATÓRIO API Anthropic
temporal_analysis:
  use_anthropic: true  # OBRIGATÓRIO a partir da Stage 3
  analysis_window_days: 7
  significance_threshold: 2.0
  event_sensitivity: 0.8
  
# Stage 11: Network Analysis - OBRIGATÓRIO API Anthropic
network_analysis:
  use_anthropic: true  # OBRIGATÓRIO a partir da Stage 3
  min_edge_weight: 3
  max_nodes: 500
  community_sample_size: 100
  
# Stage 12: Qualitative Analysis - OBRIGATÓRIO API Anthropic
qualitative:
  use_anthropic_classification: true  # OBRIGATÓRIO a partir da Stage 3
  confidence_threshold: 0.8
  conspiracy_detection: true
  negacionism_detection: true
  
# Stage 13: Pipeline Review - OBRIGATÓRIO API Anthropic
pipeline_review:
  use_anthropic: true  # OBRIGATÓRIO a partir da Stage 3
  quality_threshold: 0.8
  detail_level: "comprehensive"
  generate_recommendations: true

# Voyage.ai Embeddings Integration
embeddings:
  # Voyage.ai model for semantic analysis
  model: "voyage-3.5-lite"  # voyage-3.5-lite (recommended), voyage-3.5, voyage-large-2
  batch_size: 128
  max_tokens: 32000
  cache_embeddings: true
  similarity_threshold: 0.8
  
  # Integration settings
  integration:
    deduplication: true      # Use embeddings for semantic duplicate detection
    topic_modeling: true     # Enhance topic modeling with embeddings
    clustering: true         # Use embeddings for clustering
    tfidf_analysis: true     # Enhance TF-IDF with semantic analysis

# Hybrid Search Configuration
hybrid_search:
  max_results: 100
  dense_weight: 0.7
  sparse_weight: 0.3
  rerank_top_k: 200
  min_similarity: 0.1
  
  # FAISS Configuration
  faiss:
    index_type: "IVF"     # Options: Flat, IVF, HNSW
    nlist: 100            # Number of clusters for IVF
    nprobe: 10            # Number of clusters to search
  
  # TF-IDF Configuration for sparse search
  tfidf:
    max_features: 10000
    ngram_range: [1, 2]
    min_df: 2
    max_df: 0.95

# Semantic Search Configuration  
semantic_search:
  max_results: 100
  similarity_threshold: 0.7
  cluster_eps: 0.3
  min_cluster_size: 5
  enable_caching: true
  
# Optimized Cache Configuration
cache:
  embedding_cache:
    max_memory_mb: 256
    compression_level: 6
    ttl_hours: 24
  search_cache:
    max_memory_mb: 128
    compression_level: 6
    ttl_hours: 6
